{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AzTMYbkl7gZrlZvXJgu5abc3W8Hdelyv",
      "authorship_tag": "ABX9TyPm/lyMI9zsRHLoWzRDYk7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zeeshan138063/rag/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# RAG\n",
        "RAG (Retrieval-Augmented Generation) pipelines tackle AI hallucinations by integrating real-time information retrieval with text generation. This ensures that models produce more accurate, context-grounded responses, reducing the chances of misleading or incorrect outputs. A powerful step forward in building reliable AI!\n",
        "\n",
        "[More on RAG](https://www.linkedin.com/pulse/rag-retrieval-augmented-generation-pipelines-muhammad-zeeshan-oodvf/?trackingId=pXByMvHLQDW%2Fw11KVFg8LQ%3D%3D)\n",
        "\n",
        "\n",
        "\n",
        "1.   Ingestion\n",
        "2.   Retrieval\n",
        "3.   Synthesis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o0FxhAllEP8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "***LangChain*** is a framework for developing applications powered by large language models (LLMs).\n",
        "by providing utilities for working with text, embeddings, memory, and more."
      ],
      "metadata": {
        "id": "Kx1J-D4Xx4sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YPPw3kwZHdZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/):\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        " Loaders refer to components that are used to load or ingest data from various sources such as files, databases, APIs, or even web pages, and converting that data into a format that can be processed by LangChain.\n",
        "\n",
        "DocumentLoaders load data into the standard LangChain Document format.\n",
        "\n",
        "Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\n",
        "\n",
        "## Types of Loaders:\n",
        "\n",
        "\n",
        "*   **File Loaders:** These load data from files like text files, PDFs, CSVs, JSON, etc.\n",
        "\n",
        "*   **Database Loaders:** These load data from databases, converting rows or documents into a format that can be used by the language model.\n",
        "\n",
        "* **Web Loaders:** These scrape or fetch content from web pages, transforming the retrieved text into a structured format.\n",
        "\n",
        "* **API Loaders:** These fetch data from APIs and convert the responses into a usable format for further processing.\n",
        "\n",
        "* **Customization:**\n",
        " LangChain allows you to create custom loaders if your data source doesnâ€™t fit the pre-existing loaders. This flexibility ensures you can integrate almost any data source into your language model application.\n",
        "\n",
        "\n",
        "###  Preprocessing:\n",
        "Loader performs necessary preprocessing steps such as tokenization, normalization and format conversiotn to ensure data is in the optimal state for model consumption\n",
        "\n",
        " ### Integration:\n",
        " Once the data is loaded, it can be passed through various components of LangChain, such as text splitting, embedding generation, memory integration, or directly into a language model for processing.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Example Use Case\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "If you have a large set of PDF documents and you want to extract the text content for use in a language model application, you could use a PDF loader in LangChain to automate this process. The loader would read each PDF, extract the text, and format it in a way that the language model can use it for tasks like summarization, question answering, or information retrieval.\n",
        "\n",
        "Loaders are a crucial part of building data pipelines in LangChain, ensuring that data is efficiently and correctly ingested into the system for further processing."
      ],
      "metadata": {
        "id": "5X8H_xXU0oOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [PDF Loader](https://python.langchain.com/v0.2/docs/integrations/document_loaders/pypdfloader/)\n",
        "Lets load the PDF"
      ],
      "metadata": {
        "id": "bhe-MpTYJ0eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "JIAy51fTKECh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QImHfJNmFNk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader #Initializatio\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/Colab Notebooks/Rich-Dad-Poor-Dad.pdf\")\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "Bn8xlkO7KtIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Each page is a Document\n",
        "A Document contains text(page_content) and metadata."
      ],
      "metadata": {
        "id": "U_VnscLmNhez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEdyLwzjN1oo",
        "outputId": "0b8839b2-3bbd-4e90-d988-11f900188a24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page = pages[224]"
      ],
      "metadata": {
        "id": "1OtsVqAxN-Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(page)"
      ],
      "metadata": {
        "id": "ElEw4kvjOHPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(page)"
      ],
      "metadata": {
        "id": "ccARxQ0bPUkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.metadata"
      ],
      "metadata": {
        "id": "S9RMojHlPclZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.page_content[0:10]"
      ],
      "metadata": {
        "id": "hFgL5mJDPhVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.json()"
      ],
      "metadata": {
        "id": "10jRbKG2P_Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare 2 pages."
      ],
      "metadata": {
        "id": "QCVB6v0TQTCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages[1]==pages[-1]"
      ],
      "metadata": {
        "id": "PehaAvCSQK-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF Loader\n",
        "Lets load the PDF"
      ],
      "metadata": {
        "id": "1uEAoGtPeTCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install onnxruntime-gpu\n",
        "!pip install rapidocr-onnxruntime"
      ],
      "metadata": {
        "id": "wfT5CVCreTCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.document_loaders import PyPDFLoader\n",
        "# loader = PyPDFLoader(\"/content/drive/MyDrive/Colab Notebooks/Rich-Dad-Poor-Dad.pdf\")\n",
        "# pages = loader.load()\n",
        "docs = []\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/Colab Notebooks/Rich-Dad-Poor-Dad.pdf\", extract_images=True)\n",
        "# docs_lazy = loader.lazy_load()\n",
        "docs = await loader.aload()\n",
        "\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    docs.append(doc)"
      ],
      "metadata": {
        "id": "UvrE15YZeTCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Each page is a Document\n",
        "A Document contains text(page_content) and metadata."
      ],
      "metadata": {
        "id": "8R9SJULJeTCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8839b2-3bbd-4e90-d988-11f900188a24",
        "id": "k08YTwyGeTCU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "page = pages[224]"
      ],
      "metadata": {
        "id": "QnnvPHffeTCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(page)"
      ],
      "metadata": {
        "id": "SbG4XSc6eTCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(page)"
      ],
      "metadata": {
        "id": "Y9HaSS8ueTCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.metadata"
      ],
      "metadata": {
        "id": "vPKz5BjneTCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.page_content[0:10]"
      ],
      "metadata": {
        "id": "zDeJhrEKeTCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page.json()"
      ],
      "metadata": {
        "id": "iIx547IOeTCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare 2 pages."
      ],
      "metadata": {
        "id": "L4bzzD5VeTCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages[1]==pages[-1]"
      ],
      "metadata": {
        "id": "AW31w1uKeTCV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}